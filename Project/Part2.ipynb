{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T21:18:52.755488200Z",
     "start_time": "2024-02-01T21:18:47.028975700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='Latin', header=None)\n",
    "df = data.iloc[750000:850001]\n",
    "columns = ['Label', 'User ID', 'Datetime', 'Query', 'UserName', 'Tweet Text']\n",
    "df.columns = columns"
   ],
   "id": "b4c1a269e7d4d2e4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T21:18:55.240224700Z",
     "start_time": "2024-02-01T21:18:55.162080800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Query</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Tweet Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750000</th>\n",
       "      <td>0</td>\n",
       "      <td>2285370474</td>\n",
       "      <td>Mon Jun 22 15:02:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>idmoore</td>\n",
       "      <td>@Opotopo small slip on Tryfan few weeks back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750001</th>\n",
       "      <td>0</td>\n",
       "      <td>2285370823</td>\n",
       "      <td>Mon Jun 22 15:02:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xbeautifulmessx</td>\n",
       "      <td>@Idristwilight You can post HAN when you want....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750002</th>\n",
       "      <td>0</td>\n",
       "      <td>2285371185</td>\n",
       "      <td>Mon Jun 22 15:02:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thefirstsight</td>\n",
       "      <td>@rose_7 Ohh poor jan  please tell her that if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750003</th>\n",
       "      <td>0</td>\n",
       "      <td>2285371495</td>\n",
       "      <td>Mon Jun 22 15:02:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Sarah2713</td>\n",
       "      <td>Finally home from work...It was a looong day!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750004</th>\n",
       "      <td>0</td>\n",
       "      <td>2285371762</td>\n",
       "      <td>Mon Jun 22 15:02:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dierockerfrau</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750005</th>\n",
       "      <td>0</td>\n",
       "      <td>2285372377</td>\n",
       "      <td>Mon Jun 22 15:02:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>alexbates</td>\n",
       "      <td>I chatted with someone on the online Apple sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750006</th>\n",
       "      <td>0</td>\n",
       "      <td>2285372393</td>\n",
       "      <td>Mon Jun 22 15:02:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>captainsubtle</td>\n",
       "      <td>Back to office to empty aircon water tank  emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750007</th>\n",
       "      <td>0</td>\n",
       "      <td>2285372511</td>\n",
       "      <td>Mon Jun 22 15:02:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LizLemonCologne</td>\n",
       "      <td>@ToxicMelvin Too late  However it works now. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750008</th>\n",
       "      <td>0</td>\n",
       "      <td>2285372519</td>\n",
       "      <td>Mon Jun 22 15:02:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>esben_thomsen</td>\n",
       "      <td>@exljbris it can't connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750009</th>\n",
       "      <td>0</td>\n",
       "      <td>2285373042</td>\n",
       "      <td>Mon Jun 22 15:03:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jlcookaz</td>\n",
       "      <td>Missing my 20yr old baby-moved to WA.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label     User ID                      Datetime     Query  \\\n",
       "750000      0  2285370474  Mon Jun 22 15:02:48 PDT 2009  NO_QUERY   \n",
       "750001      0  2285370823  Mon Jun 22 15:02:49 PDT 2009  NO_QUERY   \n",
       "750002      0  2285371185  Mon Jun 22 15:02:51 PDT 2009  NO_QUERY   \n",
       "750003      0  2285371495  Mon Jun 22 15:02:52 PDT 2009  NO_QUERY   \n",
       "750004      0  2285371762  Mon Jun 22 15:02:54 PDT 2009  NO_QUERY   \n",
       "750005      0  2285372377  Mon Jun 22 15:02:57 PDT 2009  NO_QUERY   \n",
       "750006      0  2285372393  Mon Jun 22 15:02:57 PDT 2009  NO_QUERY   \n",
       "750007      0  2285372511  Mon Jun 22 15:02:57 PDT 2009  NO_QUERY   \n",
       "750008      0  2285372519  Mon Jun 22 15:02:57 PDT 2009  NO_QUERY   \n",
       "750009      0  2285373042  Mon Jun 22 15:03:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "               UserName                                         Tweet Text  \n",
       "750000          idmoore  @Opotopo small slip on Tryfan few weeks back, ...  \n",
       "750001  xbeautifulmessx  @Idristwilight You can post HAN when you want....  \n",
       "750002    thefirstsight  @rose_7 Ohh poor jan  please tell her that if ...  \n",
       "750003        Sarah2713  Finally home from work...It was a looong day!!...  \n",
       "750004    dierockerfrau                   im very sad 4 chantelle and tom   \n",
       "750005        alexbates  I chatted with someone on the online Apple sto...  \n",
       "750006    captainsubtle  Back to office to empty aircon water tank  emp...  \n",
       "750007  LizLemonCologne  @ToxicMelvin Too late  However it works now. A...  \n",
       "750008    esben_thomsen                        @exljbris it can't connect   \n",
       "750009         jlcookaz             Missing my 20yr old baby-moved to WA.   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "id": "9164822a87bf22e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ],
   "id": "78739ffea0874cb7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T21:19:00.769771200Z",
     "start_time": "2024-02-01T21:19:00.567034Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750000</th>\n",
       "      <td>0</td>\n",
       "      <td>@Opotopo small slip on Tryfan few weeks back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750001</th>\n",
       "      <td>0</td>\n",
       "      <td>@Idristwilight You can post HAN when you want....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750002</th>\n",
       "      <td>0</td>\n",
       "      <td>@rose_7 Ohh poor jan  please tell her that if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750003</th>\n",
       "      <td>0</td>\n",
       "      <td>Finally home from work...It was a looong day!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750004</th>\n",
       "      <td>0</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750005</th>\n",
       "      <td>0</td>\n",
       "      <td>I chatted with someone on the online Apple sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750006</th>\n",
       "      <td>0</td>\n",
       "      <td>Back to office to empty aircon water tank  emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750007</th>\n",
       "      <td>0</td>\n",
       "      <td>@ToxicMelvin Too late  However it works now. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750008</th>\n",
       "      <td>0</td>\n",
       "      <td>@exljbris it can't connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750009</th>\n",
       "      <td>0</td>\n",
       "      <td>Missing my 20yr old baby-moved to WA.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label                                         Tweet Text\n",
       "750000      0  @Opotopo small slip on Tryfan few weeks back, ...\n",
       "750001      0  @Idristwilight You can post HAN when you want....\n",
       "750002      0  @rose_7 Ohh poor jan  please tell her that if ...\n",
       "750003      0  Finally home from work...It was a looong day!!...\n",
       "750004      0                   im very sad 4 chantelle and tom \n",
       "750005      0  I chatted with someone on the online Apple sto...\n",
       "750006      0  Back to office to empty aircon water tank  emp...\n",
       "750007      0  @ToxicMelvin Too late  However it works now. A...\n",
       "750008      0                        @exljbris it can't connect \n",
       "750009      0             Missing my 20yr old baby-moved to WA. "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Remove duplicates and useless data\n",
    "df = df.drop(['User ID', 'Datetime', 'Query', 'UserName'], axis=1)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "\n",
    "df.head(10)"
   ],
   "id": "90017ddd53461add"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T21:19:05.538018Z",
     "start_time": "2024-02-01T21:19:01.953581800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\Desktop\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750000</th>\n",
       "      <td>0</td>\n",
       "      <td>small slip tryfan weeks back felt side pull di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750001</th>\n",
       "      <td>0</td>\n",
       "      <td>post han want great still working tld though g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750002</th>\n",
       "      <td>0</td>\n",
       "      <td>ohh poor jan please tell cans send us email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750003</th>\n",
       "      <td>0</td>\n",
       "      <td>finally home workit looong day monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750004</th>\n",
       "      <td>0</td>\n",
       "      <td>im sad 4 chantelle tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750005</th>\n",
       "      <td>0</td>\n",
       "      <td>chatted someone online apple store said would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750006</th>\n",
       "      <td>0</td>\n",
       "      <td>back office empty aircon water tank empty offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750007</th>\n",
       "      <td>0</td>\n",
       "      <td>late however works really happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750008</th>\n",
       "      <td>0</td>\n",
       "      <td>cant connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750009</th>\n",
       "      <td>0</td>\n",
       "      <td>missing 20yr old babymoved wa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label                                         Tweet Text\n",
       "750000      0  small slip tryfan weeks back felt side pull di...\n",
       "750001      0  post han want great still working tld though g...\n",
       "750002      0        ohh poor jan please tell cans send us email\n",
       "750003      0              finally home workit looong day monday\n",
       "750004      0                             im sad 4 chantelle tom\n",
       "750005      0  chatted someone online apple store said would ...\n",
       "750006      0  back office empty aircon water tank empty offi...\n",
       "750007      0                    late however works really happy\n",
       "750008      0                                       cant connect\n",
       "750009      0                      missing 20yr old babymoved wa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "  # Remove URLs\n",
    "  tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "  # Remove mentions\n",
    "  tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
    "  # Remove hashtags\n",
    "  tweet = re.sub(r\"#\\w+\", \"\", tweet)\n",
    "  # Remove CashTags\n",
    "  tweet = re.sub(r'\\$[^\\s]+', '', tweet) \n",
    "  # Remove punctuation\n",
    "  tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)\n",
    "  # Convert to lowercase\n",
    "  tweet = tweet.lower()\n",
    "  # Remove stopwords\n",
    "  tweet = tweet.split()\n",
    "  tweet = [word for word in tweet if word not in stopWords]\n",
    "  # Join words back\n",
    "  tweet = \" \".join(tweet)\n",
    "  return tweet\n",
    "\n",
    "# Apply the function to the tweet column\n",
    "df[\"Tweet Text\"] = df[\"Tweet Text\"].apply(preprocess_tweet)\n",
    "\n",
    "df.head(10)\n"
   ],
   "id": "9f9dfcbf92d3850e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T21:19:24.998052400Z",
     "start_time": "2024-02-01T21:19:05.538018Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750000</th>\n",
       "      <td>0</td>\n",
       "      <td>small slip tryfan week back felt side pull did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750001</th>\n",
       "      <td>0</td>\n",
       "      <td>post han want great still working tld though g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750002</th>\n",
       "      <td>0</td>\n",
       "      <td>ohh poor jan please tell can send u email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750003</th>\n",
       "      <td>0</td>\n",
       "      <td>finally home workit looong day monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750004</th>\n",
       "      <td>0</td>\n",
       "      <td>im sad 4 chantelle tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750005</th>\n",
       "      <td>0</td>\n",
       "      <td>chatted someone online apple store said would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750006</th>\n",
       "      <td>0</td>\n",
       "      <td>back office empty aircon water tank empty offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750007</th>\n",
       "      <td>0</td>\n",
       "      <td>late however work really happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750008</th>\n",
       "      <td>0</td>\n",
       "      <td>cant connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750009</th>\n",
       "      <td>0</td>\n",
       "      <td>missing 20yr old babymoved wa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label                                         Tweet Text\n",
       "750000      0  small slip tryfan week back felt side pull did...\n",
       "750001      0  post han want great still working tld though g...\n",
       "750002      0          ohh poor jan please tell can send u email\n",
       "750003      0              finally home workit looong day monday\n",
       "750004      0                             im sad 4 chantelle tom\n",
       "750005      0  chatted someone online apple store said would ...\n",
       "750006      0  back office empty aircon water tank empty offi...\n",
       "750007      0                     late however work really happy\n",
       "750008      0                                       cant connect\n",
       "750009      0                      missing 20yr old babymoved wa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization(tweet):\n",
    "    # Tokenize tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "    # Lemmatize the tokens and then concatenate\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_tweet = ' '.join(tokens)\n",
    "    return lemmatized_tweet\n",
    "\n",
    "df[\"Tweet Text\"] = df[\"Tweet Text\"].apply(lemmatization)\n",
    "\n",
    "df.head(10)"
   ],
   "id": "e6109f961fb8eeb6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it works correctly and comparing the above table with its previous one, if you for example look at row number 8, the 's' character from 'works' is removed."
   ],
   "id": "3813fd497c10e234"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T21:20:23.959597200Z",
     "start_time": "2024-02-01T21:20:23.654109200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ],
   "id": "df3eb48fc745e937"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ],
   "id": "5870f10e58994dee"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ],
   "id": "867edcec279d7d29"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T19:04:57.514380300Z",
     "start_time": "2024-02-01T19:04:57.387349500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for TF-IDF model are: {'logreg__C': 1, 'tfidf__max_features': 100000, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      "Validation accuracy of TF-IDF model:  0.7776102904230731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df[\"Tweet Text\"], df[\"Label\"].replace(4, 1), test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF, and logistic regression\n",
    "tfidf_PL = Pipeline([\n",
    "  (\"tfidf\", TfidfVectorizer()),\n",
    "  (\"logreg\", LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "    'tfidf__max_features': [10000, 100000],\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "\n",
    "tfidf_grid_search = GridSearchCV(tfidf_PL, param_grid, cv=5, scoring='accuracy')\n",
    "tfidf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters for each model\n",
    "print(\"Best parameters for TF-IDF model are:\", tfidf_grid_search.best_params_)\n",
    "\n",
    "# Predict the labels on the validation set\n",
    "y_val_pred = tfidf_grid_search.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the result\n",
    "print(\"Validation accuracy of TF-IDF model: \", val_accuracy)"
   ],
   "id": "f50a8018f3c54e28"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T19:05:35.859618700Z",
     "start_time": "2024-02-01T19:05:35.759333300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of TF-IDF model:  0.7819533762057878\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the grid search\n",
    "best_tfidf_model = tfidf_grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_test_pred = best_tfidf_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the result\n",
    "print(\"Test accuracy of TF-IDF model: \", test_accuracy)"
   ],
   "id": "c11c6928b755049f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T19:08:50.271927400Z",
     "start_time": "2024-02-01T19:08:47.810277400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_tfidf_model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import joblib\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_tfidf_model, \"best_tfidf_model.pkl\", compress=1)"
   ],
   "id": "3aabbfcd57d3c1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ],
   "id": "5b7bcf549e576bea"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T19:27:49.648497500Z",
     "start_time": "2024-02-01T19:27:49.517769100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for BoW model are: {'bow__max_features': 100000, 'bow__ngram_range': (1, 2), 'logreg__C': 1}\n",
      "\n",
      "Validation accuracy of BoW model:  0.7753994573409707\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with BoW, and logistic regression\n",
    "BoW_PL = Pipeline([\n",
    "  (\"bow\", CountVectorizer()),\n",
    "  (\"logreg\", LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'bow__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "    'bow__max_features': [10000, 100000],\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "\n",
    "BoW_grid_search = GridSearchCV(BoW_PL, param_grid, cv=5, scoring='accuracy')\n",
    "BoW_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters for each model\n",
    "print(\"Best parameters for BoW model are:\", BoW_grid_search.best_params_)\n",
    "\n",
    "# Predict the labels on the validation set\n",
    "y_val_pred = BoW_grid_search.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the result\n",
    "print(\"Validation accuracy of BoW model: \", val_accuracy)"
   ],
   "id": "4c6391a12dd22176"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T19:28:09.387181100Z",
     "start_time": "2024-02-01T19:28:09.318644700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of BoW model:  0.777532154340836\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the grid search\n",
    "best_BoW_model = BoW_grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_test_pred = best_BoW_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the result\n",
    "print(\"Test accuracy of BoW model: \", test_accuracy)"
   ],
   "id": "513166b689cce82e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T19:28:14.777198900Z",
     "start_time": "2024-02-01T19:28:12.217613600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_BoW_model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import joblib\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_BoW_model, \"best_BoW_model.pkl\", compress=1)"
   ],
   "id": "75966c94c89dfef8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained (HuggingFace)"
   ],
   "id": "19399069d9e79895"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T11:05:37.158158900Z",
     "start_time": "2024-02-02T11:05:36.679144600Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-02T12:34:32.701724Z",
     "iopub.status.busy": "2024-02-02T12:34:32.701055Z",
     "iopub.status.idle": "2024-02-02T12:34:32.860748Z",
     "shell.execute_reply": "2024-02-02T12:34:32.859501Z",
     "shell.execute_reply.started": "2024-02-02T12:34:32.701679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>small slip tryfan week back felt side pull did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>post han want great still working tld though g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ohh poor jan please tell can send u email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>finally home workit looong day monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>im sad 4 chantelle tom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                         Tweet Text\n",
       "0      0  small slip tryfan week back felt side pull did...\n",
       "1      0  post han want great still working tld though g...\n",
       "2      0          ohh poor jan please tell can send u email\n",
       "3      0              finally home workit looong day monday\n",
       "4      0                             im sad 4 chantelle tom"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/tweet-sentiment/data.csv', encoding='Latin')\n",
    "\n",
    "df.dropna()\n",
    "# Replace any empty strings with NaN and then remove them\n",
    "df = df.replace(\"\", np.nan).dropna()\n",
    "\n",
    "df.head()"
   ],
   "id": "eeb9a397ce7992ba"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T20:57:57.593010100Z",
     "start_time": "2024-02-01T20:57:40.437443900Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-01T22:30:25.369544Z",
     "iopub.status.busy": "2024-02-01T22:30:25.368943Z",
     "iopub.status.idle": "2024-02-01T23:28:02.852825Z",
     "shell.execute_reply": "2024-02-01T23:28:02.851664Z",
     "shell.execute_reply.started": "2024-02-01T22:30:25.369514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 22:30:27.099793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-01 22:30:27.099894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-01 22:30:27.223984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05f6bde013840149640c3d17e8c1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e8ffa1863b4172b169c60fe7374fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09a494a9cd24e79825674e3b03686dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ce253d5c3d4a17845c3f4f6a451f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56a044ecbd74b2aa90d177cab8ea1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706826773.207638      83 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4955/4955 [==============================] - 1116s 216ms/step - loss: 0.4950 - sparse_categorical_accuracy: 0.7618\n",
      "Epoch 2/3\n",
      "4955/4955 [==============================] - 1069s 216ms/step - loss: 0.4103 - sparse_categorical_accuracy: 0.8165\n",
      "Epoch 3/3\n",
      "4955/4955 [==============================] - 1071s 216ms/step - loss: 0.3360 - sparse_categorical_accuracy: 0.8600\n",
      "1239/1239 [==============================] - 87s 68ms/step - loss: 0.5269 - sparse_categorical_accuracy: 0.7877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.52691251039505, 0.7877182364463806]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Tweet Text\"], df[\"Label\"].replace(4, 1), test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the texts and convert them to tensors\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))\n",
    "\n",
    "# Fine-tune the model on the train set\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(test_dataset.batch(16))\n"
   ],
   "id": "461b3adbf2e2f944"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the sparse categorical accuracy at the end of training had reached 86% with just 3 epochs, which is so great. Also, the accuracy on test set is about 79% which is higher than the previous models.\n",
    "\n",
    "If we had more time and resources to run it on more epochs, we could get so better results :)  "
   ],
   "id": "7fef33b3a688beb3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T23:35:44.682290Z",
     "iopub.status.busy": "2024-02-01T23:35:44.681898Z",
     "iopub.status.idle": "2024-02-01T23:36:19.712639Z",
     "shell.execute_reply": "2024-02-01T23:36:19.711806Z",
     "shell.execute_reply.started": "2024-02-01T23:35:44.682260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.saved_model.save(model, 'HuggingFace-PreTrained')"
   ],
   "id": "c3356bac02ca361a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T23:50:27.515396Z",
     "iopub.status.busy": "2024-02-01T23:50:27.514757Z",
     "iopub.status.idle": "2024-02-01T23:50:30.292942Z",
     "shell.execute_reply": "2024-02-01T23:50:30.292015Z",
     "shell.execute_reply.started": "2024-02-01T23:50:27.515364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('HuggingFace-Weights')"
   ],
   "id": "30f9898876ec9fda"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggin Face (DistilBert model and twitter-roberta-base-sentiment-latest model)"
   ],
   "id": "e22b9ce594cc3e13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert"
   ],
   "id": "b906f28fd0148c58"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T11:12:37.709052900Z",
     "start_time": "2024-02-02T11:11:37.760825Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-02T12:35:26.328525Z",
     "iopub.status.busy": "2024-02-02T12:35:26.328073Z",
     "iopub.status.idle": "2024-02-02T13:28:41.307304Z",
     "shell.execute_reply": "2024-02-02T13:28:41.306138Z",
     "shell.execute_reply.started": "2024-02-02T12:35:26.328493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 12:35:28.063181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-02 12:35:28.063279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-02 12:35:28.186409: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eff978bd7c34106b651f1cc7d57b2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5b4ed2c0354b3fb392c6c7047d3507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49d2956e87744ce9847f9e20820daf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6336f98a034547b5f31c290c4e4444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aedd6ddd8a43ae9e782ef4bd3f06fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706877428.561178     107 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574/5574 [==============================] - 640s 110ms/step - loss: 0.4654 - sparse_categorical_accuracy: 0.7782\n",
      "Epoch 2/5\n",
      "5574/5574 [==============================] - 609s 109ms/step - loss: 0.3583 - sparse_categorical_accuracy: 0.8419\n",
      "Epoch 3/5\n",
      "5574/5574 [==============================] - 611s 110ms/step - loss: 0.2386 - sparse_categorical_accuracy: 0.9019\n",
      "Epoch 4/5\n",
      "5574/5574 [==============================] - 616s 110ms/step - loss: 0.1549 - sparse_categorical_accuracy: 0.9401\n",
      "Epoch 5/5\n",
      "5574/5574 [==============================] - 612s 110ms/step - loss: 0.1113 - sparse_categorical_accuracy: 0.9582\n",
      "620/620 [==============================] - 23s 34ms/step - loss: 0.8244 - sparse_categorical_accuracy: 0.7844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8243535161018372, 0.7844383716583252]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Tweet Text\"], df[\"Label\"].replace(4, 1), test_size=0.1, random_state=42)\n",
    "\n",
    "# Load the pre-trained DistilBERT model and tokenizer\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the texts and convert them to tensors\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))\n",
    "\n",
    "# Fine-tune the model on the train set\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=5, batch_size=16)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(test_dataset.batch(16))\n"
   ],
   "id": "e1fab904f8730e9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweeter Roberta"
   ],
   "id": "1a45698613e999cb"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T14:39:04.942992Z",
     "iopub.status.busy": "2024-02-02T14:39:04.942635Z",
     "iopub.status.idle": "2024-02-02T14:49:34.763229Z",
     "shell.execute_reply": "2024-02-02T14:49:34.762219Z",
     "shell.execute_reply.started": "2024-02-02T14:39:04.942964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of twitter-roberta-base-sentiment-latest model from Huggung Face on test set is: 0.7336764557473004\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "y_pred = []\n",
    "for i in list(X_test):\n",
    "    encoded_input = tokenizer(i, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    if scores[2] >= scores[0]:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy of twitter-roberta-base-sentiment-latest model from Huggung Face on test set is:\", accuracy)"
   ],
   "id": "1e136f5c1a22f0dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to the above results, my inference is:\n",
    "\n",
    "- The BoW and TF-IDF models are based on the frequency of words in the tweets, and do not capture the context or the meaning of the words. The Pre-trained BERT and Roberta models are based on pre-trained language models that can capture the context and the meaning of the words, and are fine-tuned on the tweet dataset.\n",
    "- The BoW and TF-IDF models have similar performance, with TF-IDF slightly outperforming BoW. This may be because TF-IDF assigns more weight to the words that are more informative and less common, while BoW assigns equal weight to all words. The Pre-trained BERT and Roberta models have higher performance than the BoW and TF-IDF models, with Pre-trained BERT slightly outperforming Roberta. This may be because Pre-trained BERT and Roberta can learn from a large corpus of text, and can handle the complexity and the variability of the tweets better than the BoW and TF-IDF models.\n",
    "- The BoW and TF-IDF models are simpler and faster to train and evaluate than the Pre-trained BERT and Roberta models. The Pre-trained BERT and Roberta models are more complex and require more computational resources and time to train and evaluate. The BoW and TF-IDF models may be more suitable for tasks where the data is small and simple, while the Pre-trained BERT and Roberta models may be more suitable for tasks where the data is large and complex.\n",
    "- The BoW and TF-IDF models may make mistakes when the tweets contain sarcasm, irony, negation, or slang, as they may not be able to detect the tone or the intention of the tweets. The Pre-trained BERT and Roberta models may make mistakes when the tweets contain domain-specific terms, abbreviations, or hashtags, as they may not be familiar with the vocabulary or the style of the tweets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe2ac43196a6c740"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a60a6d284bd985a6"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4387915,
     "sourceId": 7533792,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4390266,
     "sourceId": 7539626,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
